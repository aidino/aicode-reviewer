"""
LangGraph Orchestrator for AI Code Review System.

This module implements the core orchestration logic using LangGraph to coordinate
multiple agents in the code review workflow. It defines the state management,
node functions, and graph structure for the multi-agent system.
"""

from typing import Any, Dict, List, Optional, TypedDict
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END
from langgraph.graph.graph import CompiledGraph
import logging

# Configure logging
logger = logging.getLogger(__name__)


class GraphState(TypedDict):
    """
    State model for the LangGraph workflow.
    
    This TypedDict defines the shared state that flows through all nodes
    in the code review workflow. Each node can read from and update this state.
    
    Attributes:
        scan_request_data (dict): Original scan request parameters from user
        repo_url (str): Git repository URL to analyze
        pr_id (Optional[int]): Pull request ID if scanning a specific PR
        project_code (Optional[Dict[str, str]]): Full project code files (filename -> content)
        pr_diff (Optional[str]): PR diff content if scanning a specific PR
        parsed_asts (Optional[Dict[str, Any]]): Parsed ASTs for each file
        static_analysis_findings (Optional[List[dict]]): Results from static analysis
        llm_insights (Optional[str]): Insights generated by LLM analysis
        project_scan_result (Optional[dict]): Results from ProjectScanningAgent
        report_data (Optional[dict]): Final structured report data
        markdown_report (Optional[str]): Markdown formatted report
        json_report (Optional[str]): JSON formatted report
        error_message (Optional[str]): Error message if workflow fails
        current_step (str): Current step in the workflow for tracking
        workflow_metadata (dict): Additional metadata for the workflow
    """
    scan_request_data: dict
    repo_url: str
    pr_id: Optional[int]
    project_code: Optional[Dict[str, str]]
    pr_diff: Optional[str]
    parsed_asts: Optional[Dict[str, Any]]
    static_analysis_findings: Optional[List[dict]]
    llm_insights: Optional[str]
    project_scan_result: Optional[dict]
    report_data: Optional[dict]
    markdown_report: Optional[str]
    json_report: Optional[str]
    error_message: Optional[str]
    current_step: str
    workflow_metadata: dict


def start_scan(state: GraphState) -> Dict[str, Any]:
    """
    Entry point node for the code review workflow.
    
    Initializes the workflow state and validates the scan request.
    
    Args:
        state (GraphState): Current workflow state
        
    Returns:
        Dict[str, Any]: Updated state with initialization data
    """
    logger.info("Starting code review scan workflow")
    
    # TODO: Integrate with UserInteractionAgent
    # - Validate scan request parameters
    # - Set up workflow tracking
    # - Initialize logging and metrics
    
    try:
        # Extract basic parameters from scan request
        scan_data = state.get("scan_request_data", {})
        repo_url = scan_data.get("repo_url", "")
        pr_id = scan_data.get("pr_id")
        
        # Validate required parameters
        if not repo_url:
            return {
                "error_message": "Repository URL is required",
                "current_step": "error"
            }
        
        logger.info(f"Initialized scan for repo: {repo_url}, PR: {pr_id}")
        
        return {
            "repo_url": repo_url,
            "pr_id": pr_id,
            "current_step": "fetch_code",
            "workflow_metadata": {
                "start_time": "timestamp_placeholder",
                "scan_type": "pr" if pr_id else "project"
            }
        }
        
    except Exception as e:
        logger.error(f"Error in start_scan: {str(e)}")
        return {
            "error_message": f"Failed to initialize scan: {str(e)}",
            "current_step": "error"
        }


def fetch_code_node(state: GraphState) -> Dict[str, Any]:
    """
    Node for fetching code from Git repository.
    
    Retrieves either PR diff or full project code based on scan type.
    
    Args:
        state (GraphState): Current workflow state
        
    Returns:
        Dict[str, Any]: Updated state with fetched code data
    """
    logger.info("Fetching code from repository")
    
    try:
        # Import CodeFetcherAgent
        from src.core_engine.agents.code_fetcher_agent import CodeFetcherAgent
        
        # Initialize the agent
        code_fetcher = CodeFetcherAgent()
        
        repo_url = state["repo_url"]
        pr_id = state.get("pr_id")
        scan_data = state.get("scan_request_data", {})
        
        if pr_id:
            # PR-specific fetching logic
            logger.info(f"Fetching PR #{pr_id} from {repo_url}")
            
            # Get branch information from scan request
            target_branch = scan_data.get("target_branch", "main")
            source_branch = scan_data.get("source_branch")
            
            if not source_branch:
                # If source branch not provided, try common patterns
                source_branch = scan_data.get("branch", f"pr-{pr_id}")
                logger.warning(f"Source branch not specified, using: {source_branch}")
            
            try:
                # Fetch PR diff using CodeFetcherAgent
                pr_diff = code_fetcher.get_pr_diff(
                    repo_url=repo_url,
                    pr_id=pr_id,
                    target_branch=target_branch,
                    source_branch=source_branch
                )
                
                # Also get list of changed files for context
                changed_files = code_fetcher.get_changed_files_from_diff(pr_diff)
                
                return {
                    "pr_diff": pr_diff,
                    "current_step": "parse_code",
                    "workflow_metadata": {
                        **state.get("workflow_metadata", {}),
                        "changed_files": changed_files,
                        "target_branch": target_branch,
                        "source_branch": source_branch
                    }
                }
                
            except Exception as e:
                logger.error(f"Failed to fetch PR diff: {str(e)}")
                # Fallback: try to get project files from source branch
                try:
                    logger.info("Falling back to project files from source branch")
                    project_code = code_fetcher.get_project_files(repo_url, source_branch)
                    
                    return {
                        "project_code": project_code,
                        "current_step": "parse_code",
                        "workflow_metadata": {
                            **state.get("workflow_metadata", {}),
                            "fallback_mode": True,
                            "source_branch": source_branch
                        }
                    }
                except Exception as e2:
                    logger.error(f"Fallback also failed: {str(e2)}")
                    raise Exception(f"Unable to fetch PR #{pr_id}: {str(e)}")
        else:
            # Full project fetching logic
            logger.info(f"Fetching full project from {repo_url}")
            
            # Get branch information from scan request
            branch = scan_data.get("branch", "main")
            
            # Fetch project files using CodeFetcherAgent
            project_code = code_fetcher.get_project_files(
                repo_url=repo_url,
                branch_or_commit=branch
            )
            
            if not project_code:
                return {
                    "error_message": "No supported files found in the repository",
                    "current_step": "error"
                }
            
            return {
                "project_code": project_code,
                "current_step": "parse_code",
                "workflow_metadata": {
                    **state.get("workflow_metadata", {}),
                    "branch": branch,
                    "total_files": len(project_code)
                }
            }
            
    except Exception as e:
        logger.error(f"Error in fetch_code_node: {str(e)}")
        return {
            "error_message": f"Failed to fetch code: {str(e)}",
            "current_step": "error"
        }


def parse_code_node(state: GraphState) -> Dict[str, Any]:
    """
    Node for parsing source code into ASTs.
    
    Uses Tree-sitter to parse code files into Abstract Syntax Trees.
    
    Args:
        state (GraphState): Current workflow state
        
    Returns:
        Dict[str, Any]: Updated state with parsed AST data
    """
    logger.info("Parsing code into ASTs")
    
    try:
        # Import and initialize ASTParsingAgent
        from src.core_engine.agents.ast_parsing_agent import ASTParsingAgent
        
        ast_parser = ASTParsingAgent()
        
        project_code = state.get("project_code", {})
        pr_diff = state.get("pr_diff")
        
        parsed_asts = {}
        
        if pr_diff:
            # Parse PR diff content
            logger.info("Parsing PR diff content")
            
            # Extract changed files from diff and parse them
            # For now, we'll parse the diff as text and extract file information
            # In a real implementation, we'd parse the diff format to get individual files
            
            # Try to extract file content from diff
            # This is a simplified approach - in practice, you'd want more sophisticated diff parsing
            diff_lines = pr_diff.split('\n')
            current_file = None
            file_content = []
            
            for line in diff_lines:
                if line.startswith('diff --git') or line.startswith('+++'):
                    # New file detected
                    if current_file and file_content:
                        # Parse the previous file
                        content = '\n'.join(file_content)
                        if content.strip():
                            ast_node = ast_parser.parse_code_to_ast(content, 'python')
                            if ast_node:
                                parsed_asts[current_file] = {
                                    "ast_node": ast_node,
                                    "structural_info": ast_parser.extract_structural_info(ast_node, 'python')
                                }
                    
                    # Extract filename
                    if line.startswith('+++'):
                        current_file = line.split('/')[-1] if '/' in line else line.split()[-1]
                        file_content = []
                elif line.startswith('+') and not line.startswith('+++'):
                    # Added line in diff
                    file_content.append(line[1:])  # Remove the '+' prefix
            
            # Handle the last file
            if current_file and file_content:
                content = '\n'.join(file_content)
                if content.strip():
                    ast_node = ast_parser.parse_code_to_ast(content, 'python')
                    if ast_node:
                        parsed_asts[current_file] = {
                            "ast_node": ast_node,
                            "structural_info": ast_parser.extract_structural_info(ast_node, 'python')
                        }
            
            # If no files were parsed from diff, create a summary
            if not parsed_asts:
                parsed_asts = {
                    "diff_summary": {
                        "type": "diff",
                        "content": pr_diff[:1000] + "..." if len(pr_diff) > 1000 else pr_diff,
                        "note": "Could not extract individual files from diff for AST parsing"
                    }
                }
                
        elif project_code:
            # Parse full project files
            logger.info(f"Parsing {len(project_code)} project files")
            
            for filename, content in project_code.items():
                try:
                    # Detect language from filename
                    language = ast_parser._detect_language(filename)
                    
                    if language and ast_parser.is_language_supported(language):
                        # Parse the file content
                        ast_node = ast_parser.parse_code_to_ast(content, language)
                        
                        if ast_node:
                            # Extract structural information
                            structural_info = ast_parser.extract_structural_info(ast_node, language)
                            
                            parsed_asts[filename] = {
                                "language": language,
                                "ast_node": ast_node,
                                "structural_info": structural_info
                            }
                            
                            logger.debug(f"Successfully parsed {filename} ({language})")
                        else:
                            logger.warning(f"Failed to parse {filename}")
                            parsed_asts[filename] = {
                                "language": language,
                                "error": "Failed to parse AST"
                            }
                    else:
                        logger.debug(f"Skipping {filename} - unsupported language or type")
                        
                except Exception as e:
                    logger.error(f"Error parsing {filename}: {str(e)}")
                    parsed_asts[filename] = {
                        "error": str(e)
                    }
        else:
            return {
                "error_message": "No code to parse",
                "current_step": "error"
            }
        
        # Log parsing summary
        successful_parses = sum(1 for ast_data in parsed_asts.values() 
                              if isinstance(ast_data, dict) and "ast_node" in ast_data)
        logger.info(f"Successfully parsed {successful_parses}/{len(parsed_asts)} files")
        
        return {
            "parsed_asts": parsed_asts,
            "current_step": "static_analysis",
            "workflow_metadata": {
                **state.get("workflow_metadata", {}),
                "parsed_files_count": len(parsed_asts),
                "successful_parses": successful_parses
            }
        }
        
    except Exception as e:
        logger.error(f"Error in parse_code_node: {str(e)}")
        return {
            "error_message": f"Failed to parse code: {str(e)}",
            "current_step": "error"
        }


def static_analysis_node(state: GraphState) -> Dict[str, Any]:
    """
    Node for performing static analysis on parsed ASTs.
    
    Applies rule-based checks to identify potential issues using StaticAnalysisAgent.
    
    Args:
        state (GraphState): Current workflow state
        
    Returns:
        Dict[str, Any]: Updated state with static analysis findings
    """
    logger.info("Performing static analysis")
    
    try:
        # Import StaticAnalysisAgent
        from src.core_engine.agents.static_analysis_agent import StaticAnalysisAgent
        
        parsed_asts = state.get("parsed_asts", {})
        
        if not parsed_asts:
            return {
                "error_message": "No ASTs available for static analysis",
                "current_step": "error"
            }
        
        # Initialize the StaticAnalysisAgent
        static_analyzer = StaticAnalysisAgent()
        
        logger.info(f"Analyzing {len(parsed_asts)} parsed files")
        
        all_findings = []
        
        # Analyze each file's AST
        for file_path, ast_data in parsed_asts.items():
            try:
                # Extract AST node and language from parsed data
                ast_node = ast_data.get('ast_node')
                language = ast_data.get('language', 'python')
                
                if ast_node is None:
                    logger.warning(f"No AST node available for file: {file_path}")
                    continue
                
                # Perform static analysis on the file
                file_findings = static_analyzer.analyze_file_ast(
                    ast_node=ast_node,
                    file_path=file_path,
                    language=language
                )
                
                all_findings.extend(file_findings)
                logger.debug(f"Found {len(file_findings)} issues in {file_path}")
                
            except Exception as e:
                logger.error(f"Error analyzing file {file_path}: {str(e)}")
                # Continue with other files even if one fails
                continue
        
        logger.info(f"Static analysis completed. Found {len(all_findings)} total issues across all files")
        
        return {
            "static_analysis_findings": all_findings,
            "current_step": "impact_analysis"
        }
        
    except Exception as e:
        logger.error(f"Error in static_analysis_node: {str(e)}")
        return {
            "error_message": f"Failed to perform static analysis: {str(e)}",
            "current_step": "error"
        }


def impact_analysis_node(state: GraphState) -> Dict[str, Any]:
    """
    Node thực hiện phân tích tác động thay đổi (impact analysis).
    Sử dụng ImpactAnalysisAgent để xác định các thực thể bị ảnh hưởng bởi diff và dependency graph.
    """
    logger.info("Running impact analysis node")
    try:
        from src.core_engine.agents.impact_analysis.impact_analysis_agent import ImpactAnalysisAgent
        from src.core_engine.agents.impact_analysis.models import ImpactAnalysisInput

        pr_diff = state.get("pr_diff")
        workflow_metadata = state.get("workflow_metadata", {})
        changed_files = workflow_metadata.get("changed_files")
        dependency_graph = workflow_metadata.get("dependency_graph", {})
        # Nếu không có dependency_graph, tạo rỗng
        if dependency_graph is None:
            dependency_graph = {}
        # Nếu không có diff, bỏ qua node này
        if not pr_diff and not changed_files:
            logger.info("No diff or changed files, skipping impact analysis")
            return {"current_step": "llm_analysis"}
        # Chuẩn bị input cho agent
        input_data = ImpactAnalysisInput(
            diff=pr_diff or "",
            dependency_graph=dependency_graph,
            changed_files=changed_files
        )
        agent = ImpactAnalysisAgent()
        result = agent.analyze_impact(input_data)
        logger.info(f"Impact analysis found {len(result.impacted_entities)} impacted entities")
        return {
            "impact_analysis_result": result.model_dump(),
            "current_step": "llm_analysis"
        }
    except Exception as e:
        logger.error(f"Error in impact_analysis_node: {str(e)}")
        return {
            "error_message": f"Failed to perform impact analysis: {str(e)}",
            "current_step": "error"
        }


def llm_analysis_node(state: GraphState) -> Dict[str, Any]:
    """
    Node for performing LLM-based semantic analysis.
    
    Uses LLMOrchestratorAgent to provide deeper code insights through LLM analysis.
    
    Args:
        state (GraphState): Current workflow state
        
    Returns:
        Dict[str, Any]: Updated state with LLM insights
    """
    logger.info("Performing LLM analysis")
    
    try:
        # Import LLMOrchestratorAgent
        from src.core_engine.agents.llm_orchestrator_agent import LLMOrchestratorAgent
        
        parsed_asts = state.get("parsed_asts", {})
        static_findings = state.get("static_analysis_findings", [])
        project_code = state.get("project_code", {})
        pr_diff = state.get("pr_diff")
        
        if not parsed_asts and not project_code and not pr_diff:
            return {
                "error_message": "No code available for LLM analysis",
                "current_step": "error"
            }
        
        # Initialize LLMOrchestratorAgent with mock provider for now
        llm_orchestrator = LLMOrchestratorAgent(llm_provider='mock')
        
        logger.info("Generating LLM insights for code review")
        
        # Determine analysis type and prepare appropriate inputs
        if pr_diff:
            # Analyze PR diff
            logger.info("Analyzing PR diff with LLM")
            llm_insights = llm_orchestrator.analyze_pr_diff(pr_diff, static_findings)
            
        elif project_code:
            # Analyze full project files
            logger.info(f"Analyzing {len(project_code)} project files with LLM")
            llm_insights = llm_orchestrator.analyze_code_with_context(project_code, static_findings)
            
        else:
            # Fallback: analyze based on parsed ASTs
            logger.info("Analyzing parsed code with LLM")
            
            # Extract code content from parsed ASTs
            code_files = {}
            for filename, ast_data in parsed_asts.items():
                if isinstance(ast_data, dict) and "structural_info" in ast_data:
                    # Use structural info as a proxy for code content
                    structural_info = ast_data["structural_info"]
                    code_summary = f"# Structural analysis of {filename}\n"
                    code_summary += f"Classes: {len(structural_info.get('classes', []))}\n"
                    code_summary += f"Functions: {len(structural_info.get('functions', []))}\n"
                    code_summary += f"Imports: {len(structural_info.get('imports', []))}\n"
                    code_files[filename] = code_summary
            
            if code_files:
                llm_insights = llm_orchestrator.analyze_code_with_context(code_files, static_findings)
            else:
                # Basic analysis without specific code
                prompt = "Please provide general code review insights based on the static analysis findings."
                llm_insights = llm_orchestrator.invoke_llm(prompt, None, static_findings)
        
        # Log analysis summary
        findings_count = len(static_findings) if static_findings else 0
        logger.info(f"LLM analysis completed. Processed {findings_count} static analysis findings")
        
        # Determine next step based on scan type
        workflow_metadata = state.get("workflow_metadata", {})
        pr_id = state.get("pr_id")
        
        # If this is a project scan and project scanning hasn't run yet, go to project scanning
        if pr_id is None and not workflow_metadata.get("project_scan_completed", False):
            next_step = "project_scanning"
        else:
            next_step = "reporting"
        
        return {
            "llm_insights": llm_insights,
            "current_step": next_step,
            "workflow_metadata": {
                **workflow_metadata,
                "llm_provider": llm_orchestrator.llm_provider,
                "llm_model": llm_orchestrator.model_name,
                "static_findings_processed": findings_count
            }
        }
        
    except Exception as e:
        logger.error(f"Error in llm_analysis_node: {str(e)}")
        return {
            "error_message": f"Failed to perform LLM analysis: {str(e)}",
            "current_step": "error"
        }


def project_scanning_node(state: GraphState) -> Dict[str, Any]:
    """
    Node for comprehensive project-level analysis.
    
    Performs hierarchical summarization and architectural analysis of the entire project.
    Only triggered for full project scans (not PR scans).
    
    Args:
        state (GraphState): Current workflow state
        
    Returns:
        Dict[str, Any]: Updated state with project scanning results
    """
    logger.info("Starting comprehensive project scanning")
    
    try:
        # Import ProjectScanningAgent
        from src.core_engine.agents.project_scanning_agent import ProjectScanningAgent
        
        # Initialize the agent
        project_scanner = ProjectScanningAgent()
        
        # Get project code and static findings
        project_code = state.get("project_code", {})
        static_findings = state.get("static_analysis_findings", [])
        workflow_metadata = state.get("workflow_metadata", {})
        
        if not project_code:
            logger.warning("No project code available for scanning")
            return {
                "error_message": "No project code available for scanning",
                "current_step": "error"
            }
        
        # Perform comprehensive project scan
        logger.info(f"Scanning project with {len(project_code)} files")
        scan_result = project_scanner.scan_entire_project(
            code_files=project_code,
            static_findings=static_findings
        )
        
        # Add project scanning metadata
        updated_metadata = {
            **workflow_metadata,
            "project_scan_completed": True,
            "complexity_metrics": scan_result.get("complexity_metrics", {}),
            "risk_level": scan_result.get("risk_assessment", {}).get("overall_risk_level", "unknown"),
            "recommendations_count": len(scan_result.get("recommendations", []))
        }
        
        logger.info(f"Project scanning completed successfully. Risk level: {scan_result.get('risk_assessment', {}).get('overall_risk_level', 'unknown')}")
        logger.info(f"Generated {len(scan_result.get('recommendations', []))} recommendations")
        
        return {
            "project_scan_result": scan_result,
            "current_step": "reporting",
            "workflow_metadata": updated_metadata
        }
        
    except Exception as e:
        logger.error(f"Error in project_scanning_node: {str(e)}")
        return {
            "error_message": f"Failed to scan project: {str(e)}",
            "current_step": "error"
        }


def reporting_node(state: GraphState) -> Dict[str, Any]:
    """
    Node for generating the final code review report.
    
    Uses ReportingAgent to aggregate all findings into structured and formatted reports.
    
    Args:
        state (GraphState): Current workflow state
        
    Returns:
        Dict[str, Any]: Updated state with final report data
    """
    logger.info("Generating code review report")
    
    try:
        from src.core_engine.agents.reporting_agent import ReportingAgent
        static_findings = state.get("static_analysis_findings", [])
        llm_insights = state.get("llm_insights", "")
        project_scan_result = state.get("project_scan_result")
        workflow_metadata = state.get("workflow_metadata", {})
        impact_analysis_result = state.get("impact_analysis_result")
        # Prepare scan details for the report
        scan_details = {
            "repo_url": state.get("repo_url", ""),
            "pr_id": state.get("pr_id"),
            "branch": workflow_metadata.get("branch"),
            "scan_type": workflow_metadata.get("scan_type", "project"),
            "total_files": workflow_metadata.get("total_files", 0),
            "successful_parses": workflow_metadata.get("successful_parses", 0),
            "scan_id": f"scan_{int(__import__('time').time())}",
            "project_scan_result": project_scan_result,
            "impact_analysis_result": impact_analysis_result,
        }
        reporting_agent = ReportingAgent()
        logger.info(f"Generating report for {len(static_findings)} findings")
        report_data = reporting_agent.generate_report_data(
            static_findings=static_findings,
            llm_insights=llm_insights,
            scan_details=scan_details
        )
        markdown_report = reporting_agent.format_markdown_report(report_data)
        json_report = reporting_agent.export_json(report_data)
        logger.info("Code review report generated successfully")
        logger.info(f"Report contains {len(static_findings)} static analysis findings")
        logger.info(f"Markdown report: {len(markdown_report)} characters")
        logger.info(f"JSON report: {len(json_report)} characters")
        return {
            "report_data": report_data,
            "markdown_report": markdown_report,
            "json_report": json_report,
            "current_step": "completed",
            "workflow_metadata": {
                **workflow_metadata,
                "report_generation_time": __import__('datetime').datetime.now().isoformat(),
                "report_formats": ["json", "markdown"],
                "report_agent_version": reporting_agent.report_version
            }
        }
    except Exception as e:
        logger.error(f"Error in reporting_node: {str(e)}")
        return {
            "error_message": f"Failed to generate report: {str(e)}",
            "current_step": "error"
        }


def handle_error_node(state: GraphState) -> Dict[str, Any]:
    """
    Node for handling errors in the workflow.
    
    Processes errors and generates appropriate error reports.
    
    Args:
        state (GraphState): Current workflow state
        
    Returns:
        Dict[str, Any]: Updated state with error handling results
    """
    error_message = state.get("error_message", "Unknown error occurred")
    logger.error(f"Handling workflow error: {error_message}")
    
    # TODO: Implement comprehensive error handling
    # - Log detailed error information
    # - Generate user-friendly error messages
    # - Implement retry logic for transient errors
    # - Send error notifications if configured
    # - Clean up any partial state or resources
    
    error_report = {
        "summary": {
            "status": "error",
            "error_message": error_message,
            "repository": state.get("repo_url", ""),
            "pr_id": state.get("pr_id")
        },
        "error_details": {
            "step": state.get("current_step", "unknown"),
            "timestamp": "timestamp_placeholder",
            "workflow_metadata": state.get("workflow_metadata", {})
        }
    }
    
    return {
        "report_data": error_report,
        "current_step": "error_handled"
    }


def should_fetch_pr_or_project(state: GraphState) -> str:
    """
    Conditional edge function to determine scan type.
    
    Args:
        state (GraphState): Current workflow state
        
    Returns:
        str: Next node name based on scan type
    """
    if state.get("current_step") == "error":
        return "handle_error"
    
    # All code fetching goes through the same node
    # The node itself handles PR vs project logic
    return "fetch_code"


def should_continue_or_error(state: GraphState) -> str:
    """
    Conditional edge function to check for errors.
    
    Args:
        state (GraphState): Current workflow state
        
    Returns:
        str: Next node name or END
    """
    current_step = state.get("current_step", "")
    
    if current_step == "error":
        return "handle_error"
    elif current_step == "completed":
        return END
    elif current_step == "error_handled":
        return END
    elif current_step == "fetch_code":
        return "fetch_code"
    elif current_step == "parse_code":
        return "parse_code"
    elif current_step == "static_analysis":
        return "static_analysis"
    elif current_step == "impact_analysis":
        return "impact_analysis"
    elif current_step == "llm_analysis":
        return "llm_analysis"
    elif current_step == "project_scanning":
        return "project_scanning"
    elif current_step == "reporting":
        return "reporting"
    else:
        return "handle_error"


def should_run_project_scanning(state: GraphState) -> str:
    """
    Conditional edge function to determine if project scanning should run.
    
    Project scanning is only performed for full project scans (not PR scans).
    
    Args:
        state (GraphState): Current workflow state
        
    Returns:
        str: Next node name (project_scanning or llm_analysis)
    """
    if state.get("current_step") == "error":
        return "handle_error"
    
    # Check if this is a full project scan (no PR ID)
    pr_id = state.get("pr_id")
    project_code = state.get("project_code", {})
    
    # Run project scanning if:
    # 1. No PR ID (full project scan)
    # 2. Has project code to analyze
    # 3. Not an error state
    if pr_id is None and project_code:
        logger.info("Running project scanning for full project analysis")
        return "project_scanning"
    else:
        logger.info("Skipping project scanning (PR scan or no project code)")
        return "llm_analysis"


def compile_graph() -> CompiledGraph:
    """
    Compile the LangGraph workflow.
    
    Creates and configures the complete workflow graph with all nodes,
    edges, and conditional logic.
    
    Returns:
        CompiledGraph: Compiled LangGraph application ready for execution
    """
    logger.info("Compiling LangGraph workflow")
    
    # Initialize the StateGraph with our GraphState
    workflow = StateGraph(GraphState)
    
    # Add all nodes to the graph
    workflow.add_node("start_scan", start_scan)
    workflow.add_node("fetch_code", fetch_code_node)
    workflow.add_node("parse_code", parse_code_node)
    workflow.add_node("static_analysis", static_analysis_node)
    workflow.add_node("impact_analysis", impact_analysis_node)
    workflow.add_node("llm_analysis", llm_analysis_node)
    workflow.add_node("project_scanning", project_scanning_node)
    workflow.add_node("reporting", reporting_node)
    workflow.add_node("handle_error", handle_error_node)
    
    # Set entry point
    workflow.set_entry_point("start_scan")
    
    # Add edges between nodes
    workflow.add_conditional_edges(
        "start_scan",
        should_fetch_pr_or_project,
        {
            "fetch_code": "fetch_code",
            "handle_error": "handle_error"
        }
    )
    
    # Add conditional edges for basic workflow steps
    for node_name in ["fetch_code", "parse_code"]:
        workflow.add_conditional_edges(
            node_name,
            should_continue_or_error,
            {
                "fetch_code": "fetch_code",
                "parse_code": "parse_code", 
                "static_analysis": "static_analysis",
                "handle_error": "handle_error",
                END: END
            }
        )
    
    # Add special conditional edge from static_analysis to determine if project scanning should run
    workflow.add_conditional_edges(
        "static_analysis",
        should_run_project_scanning,
        {
            "project_scanning": "project_scanning",
            "llm_analysis": "llm_analysis",
            "handle_error": "handle_error"
        }
    )
    
    # Add conditional edges for analysis and reporting steps
    for node_name in ["impact_analysis", "llm_analysis", "project_scanning", "reporting"]:
        workflow.add_conditional_edges(
            node_name,
            should_continue_or_error,
            {
                "impact_analysis": "impact_analysis",
                "llm_analysis": "llm_analysis",
                "project_scanning": "project_scanning",
                "reporting": "reporting",
                "handle_error": "handle_error",
                END: END
            }
        )
    
    # Error handling node leads to END
    workflow.add_edge("handle_error", END)
    
    # Compile the graph
    app = workflow.compile()
    
    logger.info("LangGraph workflow compiled successfully")
    return app


# Example usage and testing function
def create_sample_scan_request() -> GraphState:
    """
    Create a sample scan request for testing.
    
    Returns:
        GraphState: Sample initial state for testing
    """
    return GraphState(
        scan_request_data={
            "repo_url": "https://github.com/example/test-repo",
            "pr_id": 123,
            "scan_type": "pr"
        },
        repo_url="",
        pr_id=None,
        project_code=None,
        pr_diff=None,
        parsed_asts=None,
        static_analysis_findings=None,
        llm_insights=None,
        report_data=None,
        markdown_report=None,
        json_report=None,
        error_message=None,
        current_step="start",
        workflow_metadata={}
    )


if __name__ == "__main__":
    # Example usage for testing
    logging.basicConfig(level=logging.INFO)
    
    # Compile the graph
    app = compile_graph()
    
    # Create sample input
    initial_state = create_sample_scan_request()
    
    # Run the workflow (commented out for now since agents aren't implemented)
    # result = app.invoke(initial_state)
    # print("Workflow completed:", result)
    
    print("LangGraph orchestrator compiled successfully!")
    print("Ready for integration with agent implementations.") 