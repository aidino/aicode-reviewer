"""
LangGraph Orchestrator for AI Code Review System.

This module implements the core orchestration logic using LangGraph to coordinate
multiple agents in the code review workflow. It defines the state management,
node functions, and graph structure for the multi-agent system.
"""

from typing import Any, Dict, List, Optional, TypedDict
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END
from langgraph.graph.graph import CompiledGraph
import logging

# Configure logging
logger = logging.getLogger(__name__)


class GraphState(TypedDict):
    """
    State model for the LangGraph workflow.
    
    This TypedDict defines the shared state that flows through all nodes
    in the code review workflow. Each node can read from and update this state.
    
    Attributes:
        scan_request_data (dict): Original scan request parameters from user
        repo_url (str): Git repository URL to analyze
        pr_id (Optional[int]): Pull request ID if scanning a specific PR
        project_code (Optional[Dict[str, str]]): Full project code files (filename -> content)
        pr_diff (Optional[str]): PR diff content if scanning a specific PR
        parsed_asts (Optional[Dict[str, Any]]): Parsed ASTs for each file
        static_analysis_findings (Optional[List[dict]]): Results from static analysis
        llm_insights (Optional[str]): Insights generated by LLM analysis
        report_data (Optional[dict]): Final structured report data
        error_message (Optional[str]): Error message if workflow fails
        current_step (str): Current step in the workflow for tracking
        workflow_metadata (dict): Additional metadata for the workflow
    """
    scan_request_data: dict
    repo_url: str
    pr_id: Optional[int]
    project_code: Optional[Dict[str, str]]
    pr_diff: Optional[str]
    parsed_asts: Optional[Dict[str, Any]]
    static_analysis_findings: Optional[List[dict]]
    llm_insights: Optional[str]
    report_data: Optional[dict]
    error_message: Optional[str]
    current_step: str
    workflow_metadata: dict


def start_scan(state: GraphState) -> Dict[str, Any]:
    """
    Entry point node for the code review workflow.
    
    Initializes the workflow state and validates the scan request.
    
    Args:
        state (GraphState): Current workflow state
        
    Returns:
        Dict[str, Any]: Updated state with initialization data
    """
    logger.info("Starting code review scan workflow")
    
    # TODO: Integrate with UserInteractionAgent
    # - Validate scan request parameters
    # - Set up workflow tracking
    # - Initialize logging and metrics
    
    try:
        # Extract basic parameters from scan request
        scan_data = state.get("scan_request_data", {})
        repo_url = scan_data.get("repo_url", "")
        pr_id = scan_data.get("pr_id")
        
        # Validate required parameters
        if not repo_url:
            return {
                "error_message": "Repository URL is required",
                "current_step": "error"
            }
        
        logger.info(f"Initialized scan for repo: {repo_url}, PR: {pr_id}")
        
        return {
            "repo_url": repo_url,
            "pr_id": pr_id,
            "current_step": "fetch_code",
            "workflow_metadata": {
                "start_time": "timestamp_placeholder",
                "scan_type": "pr" if pr_id else "project"
            }
        }
        
    except Exception as e:
        logger.error(f"Error in start_scan: {str(e)}")
        return {
            "error_message": f"Failed to initialize scan: {str(e)}",
            "current_step": "error"
        }


def fetch_code_node(state: GraphState) -> Dict[str, Any]:
    """
    Node for fetching code from Git repository.
    
    Retrieves either PR diff or full project code based on scan type.
    
    Args:
        state (GraphState): Current workflow state
        
    Returns:
        Dict[str, Any]: Updated state with fetched code data
    """
    logger.info("Fetching code from repository")
    
    try:
        # Import CodeFetcherAgent
        from src.core_engine.agents.code_fetcher_agent import CodeFetcherAgent
        
        # Initialize the agent
        code_fetcher = CodeFetcherAgent()
        
        repo_url = state["repo_url"]
        pr_id = state.get("pr_id")
        scan_data = state.get("scan_request_data", {})
        
        if pr_id:
            # PR-specific fetching logic
            logger.info(f"Fetching PR #{pr_id} from {repo_url}")
            
            # Get branch information from scan request
            target_branch = scan_data.get("target_branch", "main")
            source_branch = scan_data.get("source_branch")
            
            if not source_branch:
                # If source branch not provided, try common patterns
                source_branch = scan_data.get("branch", f"pr-{pr_id}")
                logger.warning(f"Source branch not specified, using: {source_branch}")
            
            try:
                # Fetch PR diff using CodeFetcherAgent
                pr_diff = code_fetcher.get_pr_diff(
                    repo_url=repo_url,
                    pr_id=pr_id,
                    target_branch=target_branch,
                    source_branch=source_branch
                )
                
                # Also get list of changed files for context
                changed_files = code_fetcher.get_changed_files_from_diff(pr_diff)
                
                return {
                    "pr_diff": pr_diff,
                    "current_step": "parse_code",
                    "workflow_metadata": {
                        **state.get("workflow_metadata", {}),
                        "changed_files": changed_files,
                        "target_branch": target_branch,
                        "source_branch": source_branch
                    }
                }
                
            except Exception as e:
                logger.error(f"Failed to fetch PR diff: {str(e)}")
                # Fallback: try to get project files from source branch
                try:
                    logger.info("Falling back to project files from source branch")
                    project_code = code_fetcher.get_project_files(repo_url, source_branch)
                    
                    return {
                        "project_code": project_code,
                        "current_step": "parse_code",
                        "workflow_metadata": {
                            **state.get("workflow_metadata", {}),
                            "fallback_mode": True,
                            "source_branch": source_branch
                        }
                    }
                except Exception as e2:
                    logger.error(f"Fallback also failed: {str(e2)}")
                    raise Exception(f"Unable to fetch PR #{pr_id}: {str(e)}")
        else:
            # Full project fetching logic
            logger.info(f"Fetching full project from {repo_url}")
            
            # Get branch information from scan request
            branch = scan_data.get("branch", "main")
            
            # Fetch project files using CodeFetcherAgent
            project_code = code_fetcher.get_project_files(
                repo_url=repo_url,
                branch_or_commit=branch
            )
            
            if not project_code:
                return {
                    "error_message": "No supported files found in the repository",
                    "current_step": "error"
                }
            
            return {
                "project_code": project_code,
                "current_step": "parse_code",
                "workflow_metadata": {
                    **state.get("workflow_metadata", {}),
                    "branch": branch,
                    "total_files": len(project_code)
                }
            }
            
    except Exception as e:
        logger.error(f"Error in fetch_code_node: {str(e)}")
        return {
            "error_message": f"Failed to fetch code: {str(e)}",
            "current_step": "error"
        }


def parse_code_node(state: GraphState) -> Dict[str, Any]:
    """
    Node for parsing source code into ASTs.
    
    Uses Tree-sitter to parse code files into Abstract Syntax Trees.
    
    Args:
        state (GraphState): Current workflow state
        
    Returns:
        Dict[str, Any]: Updated state with parsed AST data
    """
    logger.info("Parsing code into ASTs")
    
    try:
        # Import and initialize ASTParsingAgent
        from src.core_engine.agents.ast_parsing_agent import ASTParsingAgent
        
        ast_parser = ASTParsingAgent()
        
        project_code = state.get("project_code", {})
        pr_diff = state.get("pr_diff")
        
        parsed_asts = {}
        
        if pr_diff:
            # Parse PR diff content
            logger.info("Parsing PR diff content")
            
            # Extract changed files from diff and parse them
            # For now, we'll parse the diff as text and extract file information
            # In a real implementation, we'd parse the diff format to get individual files
            
            # Try to extract file content from diff
            # This is a simplified approach - in practice, you'd want more sophisticated diff parsing
            diff_lines = pr_diff.split('\n')
            current_file = None
            file_content = []
            
            for line in diff_lines:
                if line.startswith('diff --git') or line.startswith('+++'):
                    # New file detected
                    if current_file and file_content:
                        # Parse the previous file
                        content = '\n'.join(file_content)
                        if content.strip():
                            ast_node = ast_parser.parse_code_to_ast(content, 'python')
                            if ast_node:
                                parsed_asts[current_file] = {
                                    "ast_node": ast_node,
                                    "structural_info": ast_parser.extract_structural_info(ast_node, 'python')
                                }
                    
                    # Extract filename
                    if line.startswith('+++'):
                        current_file = line.split('/')[-1] if '/' in line else line.split()[-1]
                        file_content = []
                elif line.startswith('+') and not line.startswith('+++'):
                    # Added line in diff
                    file_content.append(line[1:])  # Remove the '+' prefix
            
            # Handle the last file
            if current_file and file_content:
                content = '\n'.join(file_content)
                if content.strip():
                    ast_node = ast_parser.parse_code_to_ast(content, 'python')
                    if ast_node:
                        parsed_asts[current_file] = {
                            "ast_node": ast_node,
                            "structural_info": ast_parser.extract_structural_info(ast_node, 'python')
                        }
            
            # If no files were parsed from diff, create a summary
            if not parsed_asts:
                parsed_asts = {
                    "diff_summary": {
                        "type": "diff",
                        "content": pr_diff[:1000] + "..." if len(pr_diff) > 1000 else pr_diff,
                        "note": "Could not extract individual files from diff for AST parsing"
                    }
                }
                
        elif project_code:
            # Parse full project files
            logger.info(f"Parsing {len(project_code)} project files")
            
            for filename, content in project_code.items():
                try:
                    # Detect language from filename
                    language = ast_parser._detect_language(filename)
                    
                    if language and ast_parser.is_language_supported(language):
                        # Parse the file content
                        ast_node = ast_parser.parse_code_to_ast(content, language)
                        
                        if ast_node:
                            # Extract structural information
                            structural_info = ast_parser.extract_structural_info(ast_node, language)
                            
                            parsed_asts[filename] = {
                                "language": language,
                                "ast_node": ast_node,
                                "structural_info": structural_info
                            }
                            
                            logger.debug(f"Successfully parsed {filename} ({language})")
                        else:
                            logger.warning(f"Failed to parse {filename}")
                            parsed_asts[filename] = {
                                "language": language,
                                "error": "Failed to parse AST"
                            }
                    else:
                        logger.debug(f"Skipping {filename} - unsupported language or type")
                        
                except Exception as e:
                    logger.error(f"Error parsing {filename}: {str(e)}")
                    parsed_asts[filename] = {
                        "error": str(e)
                    }
        else:
            return {
                "error_message": "No code to parse",
                "current_step": "error"
            }
        
        # Log parsing summary
        successful_parses = sum(1 for ast_data in parsed_asts.values() 
                              if isinstance(ast_data, dict) and "ast_node" in ast_data)
        logger.info(f"Successfully parsed {successful_parses}/{len(parsed_asts)} files")
        
        return {
            "parsed_asts": parsed_asts,
            "current_step": "static_analysis",
            "workflow_metadata": {
                **state.get("workflow_metadata", {}),
                "parsed_files_count": len(parsed_asts),
                "successful_parses": successful_parses
            }
        }
        
    except Exception as e:
        logger.error(f"Error in parse_code_node: {str(e)}")
        return {
            "error_message": f"Failed to parse code: {str(e)}",
            "current_step": "error"
        }


def static_analysis_node(state: GraphState) -> Dict[str, Any]:
    """
    Node for performing static analysis on parsed ASTs.
    
    Applies rule-based checks to identify potential issues.
    
    Args:
        state (GraphState): Current workflow state
        
    Returns:
        Dict[str, Any]: Updated state with static analysis findings
    """
    logger.info("Performing static analysis")
    
    # TODO: Integrate with StaticAnalysisAgent
    # - Apply Tree-sitter queries for pattern matching
    # - Check for common issues: unused imports, style violations, etc.
    # - Implement configurable rule engine
    # - Generate structured findings with severity levels
    # - Include code locations and suggested fixes
    
    try:
        parsed_asts = state.get("parsed_asts", {})
        
        if not parsed_asts:
            return {
                "error_message": "No ASTs available for static analysis",
                "current_step": "error"
            }
        
        # Placeholder static analysis logic
        logger.info(f"Analyzing {len(parsed_asts)} parsed files")
        
        # Simulate finding some issues
        static_findings = [
            {
                "type": "unused_import",
                "severity": "warning",
                "file": "main.py",
                "line": 5,
                "message": "Unused import 'os'",
                "suggestion": "Remove unused import"
            },
            {
                "type": "style_violation",
                "severity": "info",
                "file": "utils.py",
                "line": 12,
                "message": "Line too long (>88 characters)",
                "suggestion": "Break line or refactor"
            }
        ]
        
        return {
            "static_analysis_findings": static_findings,
            "current_step": "llm_analysis"
        }
        
    except Exception as e:
        logger.error(f"Error in static_analysis_node: {str(e)}")
        return {
            "error_message": f"Failed to perform static analysis: {str(e)}",
            "current_step": "error"
        }


def llm_analysis_node(state: GraphState) -> Dict[str, Any]:
    """
    Node for performing LLM-based semantic analysis.
    
    Uses Large Language Models to provide deeper code insights.
    
    Args:
        state (GraphState): Current workflow state
        
    Returns:
        Dict[str, Any]: Updated state with LLM insights
    """
    logger.info("Performing LLM analysis")
    
    # TODO: Integrate with LLMOrchestratorAgent
    # - Prepare code context for LLM analysis
    # - Generate prompts for semantic analysis
    # - Call local or remote LLM APIs
    # - Parse and structure LLM responses
    # - Combine with static analysis findings
    # - Handle LLM errors and fallbacks
    
    try:
        parsed_asts = state.get("parsed_asts", {})
        static_findings = state.get("static_analysis_findings", [])
        
        if not parsed_asts:
            return {
                "error_message": "No code available for LLM analysis",
                "current_step": "error"
            }
        
        # Placeholder LLM analysis logic
        logger.info("Generating LLM insights for code review")
        
        # Simulate LLM analysis results
        llm_insights = """
        # LLM Analysis Results (Placeholder)
        
        ## Code Quality Assessment
        - Overall code structure appears well-organized
        - Potential performance improvements identified in utils.py
        - Consider adding error handling in main.py
        
        ## Security Considerations
        - No obvious security vulnerabilities detected
        - Recommend input validation for user-facing functions
        
        ## Architectural Suggestions
        - Consider separating concerns in main.py
        - Utility functions could benefit from better documentation
        """
        
        return {
            "llm_insights": llm_insights,
            "current_step": "reporting"
        }
        
    except Exception as e:
        logger.error(f"Error in llm_analysis_node: {str(e)}")
        return {
            "error_message": f"Failed to perform LLM analysis: {str(e)}",
            "current_step": "error"
        }


def reporting_node(state: GraphState) -> Dict[str, Any]:
    """
    Node for generating the final code review report.
    
    Aggregates all findings into a structured report.
    
    Args:
        state (GraphState): Current workflow state
        
    Returns:
        Dict[str, Any]: Updated state with final report data
    """
    logger.info("Generating code review report")
    
    # TODO: Integrate with ReportingAgent
    # - Aggregate static analysis and LLM findings
    # - Generate structured report data
    # - Create Markdown/HTML formatted reports
    # - Include actionable recommendations
    # - Generate summary statistics
    # - Prepare data for Web App display
    
    try:
        static_findings = state.get("static_analysis_findings", [])
        llm_insights = state.get("llm_insights", "")
        workflow_metadata = state.get("workflow_metadata", {})
        
        # Generate comprehensive report
        report_data = {
            "summary": {
                "total_issues": len(static_findings),
                "scan_type": workflow_metadata.get("scan_type", "unknown"),
                "repository": state.get("repo_url", ""),
                "pr_id": state.get("pr_id")
            },
            "static_analysis": {
                "findings": static_findings,
                "categories": {
                    "warnings": len([f for f in static_findings if f.get("severity") == "warning"]),
                    "info": len([f for f in static_findings if f.get("severity") == "info"]),
                    "errors": len([f for f in static_findings if f.get("severity") == "error"])
                }
            },
            "llm_analysis": {
                "insights": llm_insights,
                "recommendations": "# TODO: Extract structured recommendations from LLM insights"
            },
            "metadata": {
                **workflow_metadata,
                "completion_time": "timestamp_placeholder"
            }
        }
        
        logger.info("Code review report generated successfully")
        
        return {
            "report_data": report_data,
            "current_step": "completed"
        }
        
    except Exception as e:
        logger.error(f"Error in reporting_node: {str(e)}")
        return {
            "error_message": f"Failed to generate report: {str(e)}",
            "current_step": "error"
        }


def handle_error_node(state: GraphState) -> Dict[str, Any]:
    """
    Node for handling errors in the workflow.
    
    Processes errors and generates appropriate error reports.
    
    Args:
        state (GraphState): Current workflow state
        
    Returns:
        Dict[str, Any]: Updated state with error handling results
    """
    error_message = state.get("error_message", "Unknown error occurred")
    logger.error(f"Handling workflow error: {error_message}")
    
    # TODO: Implement comprehensive error handling
    # - Log detailed error information
    # - Generate user-friendly error messages
    # - Implement retry logic for transient errors
    # - Send error notifications if configured
    # - Clean up any partial state or resources
    
    error_report = {
        "summary": {
            "status": "error",
            "error_message": error_message,
            "repository": state.get("repo_url", ""),
            "pr_id": state.get("pr_id")
        },
        "error_details": {
            "step": state.get("current_step", "unknown"),
            "timestamp": "timestamp_placeholder",
            "workflow_metadata": state.get("workflow_metadata", {})
        }
    }
    
    return {
        "report_data": error_report,
        "current_step": "error_handled"
    }


def should_fetch_pr_or_project(state: GraphState) -> str:
    """
    Conditional edge function to determine scan type.
    
    Args:
        state (GraphState): Current workflow state
        
    Returns:
        str: Next node name based on scan type
    """
    if state.get("current_step") == "error":
        return "handle_error"
    
    # All code fetching goes through the same node
    # The node itself handles PR vs project logic
    return "fetch_code"


def should_continue_or_error(state: GraphState) -> str:
    """
    Conditional edge function to check for errors.
    
    Args:
        state (GraphState): Current workflow state
        
    Returns:
        str: Next node name or END
    """
    current_step = state.get("current_step", "")
    
    if current_step == "error":
        return "handle_error"
    elif current_step == "completed":
        return END
    elif current_step == "error_handled":
        return END
    elif current_step == "fetch_code":
        return "fetch_code"
    elif current_step == "parse_code":
        return "parse_code"
    elif current_step == "static_analysis":
        return "static_analysis"
    elif current_step == "llm_analysis":
        return "llm_analysis"
    elif current_step == "reporting":
        return "reporting"
    else:
        return "handle_error"


def compile_graph() -> CompiledGraph:
    """
    Compile the LangGraph workflow.
    
    Creates and configures the complete workflow graph with all nodes,
    edges, and conditional logic.
    
    Returns:
        CompiledGraph: Compiled LangGraph application ready for execution
    """
    logger.info("Compiling LangGraph workflow")
    
    # Initialize the StateGraph with our GraphState
    workflow = StateGraph(GraphState)
    
    # Add all nodes to the graph
    workflow.add_node("start_scan", start_scan)
    workflow.add_node("fetch_code", fetch_code_node)
    workflow.add_node("parse_code", parse_code_node)
    workflow.add_node("static_analysis", static_analysis_node)
    workflow.add_node("llm_analysis", llm_analysis_node)
    workflow.add_node("reporting", reporting_node)
    workflow.add_node("handle_error", handle_error_node)
    
    # Set entry point
    workflow.set_entry_point("start_scan")
    
    # Add edges between nodes
    workflow.add_conditional_edges(
        "start_scan",
        should_fetch_pr_or_project,
        {
            "fetch_code": "fetch_code",
            "handle_error": "handle_error"
        }
    )
    
    # Add conditional edges for each step to handle errors and progression
    for node_name in ["fetch_code", "parse_code", "static_analysis", "llm_analysis", "reporting"]:
        workflow.add_conditional_edges(
            node_name,
            should_continue_or_error,
            {
                "fetch_code": "fetch_code",
                "parse_code": "parse_code", 
                "static_analysis": "static_analysis",
                "llm_analysis": "llm_analysis",
                "reporting": "reporting",
                "handle_error": "handle_error",
                END: END
            }
        )
    
    # Error handling node leads to END
    workflow.add_edge("handle_error", END)
    
    # Compile the graph
    app = workflow.compile()
    
    logger.info("LangGraph workflow compiled successfully")
    return app


# Example usage and testing function
def create_sample_scan_request() -> GraphState:
    """
    Create a sample scan request for testing.
    
    Returns:
        GraphState: Sample initial state for testing
    """
    return GraphState(
        scan_request_data={
            "repo_url": "https://github.com/example/test-repo",
            "pr_id": 123,
            "scan_type": "pr"
        },
        repo_url="",
        pr_id=None,
        project_code=None,
        pr_diff=None,
        parsed_asts=None,
        static_analysis_findings=None,
        llm_insights=None,
        report_data=None,
        error_message=None,
        current_step="start",
        workflow_metadata={}
    )


if __name__ == "__main__":
    # Example usage for testing
    logging.basicConfig(level=logging.INFO)
    
    # Compile the graph
    app = compile_graph()
    
    # Create sample input
    initial_state = create_sample_scan_request()
    
    # Run the workflow (commented out for now since agents aren't implemented)
    # result = app.invoke(initial_state)
    # print("Workflow completed:", result)
    
    print("LangGraph orchestrator compiled successfully!")
    print("Ready for integration with agent implementations.") 